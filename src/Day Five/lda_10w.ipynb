{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition  import LatentDirichletAllocation \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from doc_pb2 import Document, Dictionary\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pickle\n",
    "import jieba\n",
    "import lmdb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_all_texts(lmdb_path: str, l):\n",
    "    db = lmdb.open(lmdb_path)\n",
    "    with db.begin() as cur:\n",
    "        docs = []\n",
    "        for k, v in cur.cursor():\n",
    "            doc = Document()\n",
    "            doc.ParseFromString(v)\n",
    "            docs.append(doc.title + doc.content)\n",
    "            del doc\n",
    "            if len(docs) ==l:\n",
    "                break\n",
    "    db.close()\n",
    "    return docs\n",
    "\n",
    "def read_all_indexs(lmdb_path: str, l):\n",
    "    db = lmdb.open(lmdb_path)\n",
    "    with db.begin() as cur:\n",
    "        docs = []\n",
    "        for k, v in cur.cursor():\n",
    "            docs.append(k)\n",
    "            if len(docs) ==l:\n",
    "                break\n",
    "    db.close()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load stopwords\n",
    "stopwords_dir_path = r'C:\\Users\\zjxua\\GitHub\\CAS-NLP\\data\\stopwords'\n",
    "stopwords_filelist = [os.path.join(stopwords_dir_path, p) for p in os.listdir(stopwords_dir_path)]\n",
    "stopwords = list(reduce(lambda x, y: x + y, [open(p).read().split('\\n') for p in stopwords_filelist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_texts = read_all_texts('../../data/news data/documents', 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 46981)\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# features\n",
    "tf_vectorizer = CountVectorizer(tokenizer=lambda text: jieba.lcut(text), max_df=0.9, min_df=20, stop_words=stopwords)\n",
    "    \n",
    "tf_matrix = tf_vectorizer.fit_transform(all_texts)\n",
    "print(tf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6h 53min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=1000, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "docres = lda.fit_transform(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.save('dumps/doc2topic.npy', docres, allow_pickle=True, fix_imports=True)\n",
    "np.save('dumps/topic2word.npy', lda.components_, allow_pickle=True, fix_imports=True)\n",
    "# Wall time: 12.7 s\n",
    "with open('dumps/lda.pickle', 'wb') as f:\n",
    "    pickle.dump(file=f, obj=lda)\n",
    "# Wall time: 114 ms\n",
    "with open ('dumps/feature_names.txt', 'w', encoding='utf-8') as f:\n",
    "    for name in tf_vectorizer.get_feature_names():\n",
    "        f.write(name)\n",
    "        f.write('\\n')\n",
    "# Wall time: 5.87 s\n",
    "indexs = read_all_indexs('../../data/news data/documents', 100000)\n",
    "# Wall time: 121 ms\n",
    "with open ('dumps/indexs.txt', 'w', encoding='utf-8') as f:\n",
    "    for index in indexs:\n",
    "        f.write(index.decode('utf-8'))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 551 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2topic = np.load('dumps/doc2topic.npy')\n",
    "topic2word = np.load('dumps/topic2word.npy')\n",
    "with open ('dumps/feature_names.txt', 'r', encoding='utf-8') as f:\n",
    "    feature_names = [line for line in f]\n",
    "with open ('dumps/indexs.txt', 'r', encoding='utf-8') as f:\n",
    "    urls = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:24<00:00, 41.29it/s]\n"
     ]
    }
   ],
   "source": [
    "my_dic = Dictionary()\n",
    "# 对每一个topic，找出前1000个最大概率的doc并保存\n",
    "_, topic_num = doc2topic.shape\n",
    "for topic in tqdm(range(topic_num)):\n",
    "    topic2doc = my_dic.topic2doc.get_or_create(topic)\n",
    "    topic2doc.topic_code = topic\n",
    "    \n",
    "    topic_vec = doc2topic[:, topic]\n",
    "    largest_1000_indexs = topic_vec.argsort()[::-1][:1000]\n",
    "    for rank, index in enumerate(largest_1000_indexs):\n",
    "        docs_with_proba = topic2doc.docs.get_or_create(rank)\n",
    "        docs_with_proba.proba = topic_vec[index]\n",
    "        docs_with_proba.url = urls[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open ('dumps/dict.dump', 'wb') as f:\n",
    "    f.write(my_dic.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parse_dict = Dictionary()\n",
    "with open ('dumps/dict.dump', 'rb') as f:\n",
    "    parse_dict.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46981/46981 [01:31<00:00, 514.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# 对每一个word，找出前100个最大概率的topic并保存\n",
    "_, word_num = topic2word.shape\n",
    "for word_index in tqdm(range(word_num)):\n",
    "    word = feature_names[word_index].replace('\\n', '')\n",
    "    \n",
    "    word2topic = parse_dict.word2topic.get_or_create(word)\n",
    "    word2topic.word = word\n",
    "    \n",
    "    word_vec = topic2word[:, word_index]\n",
    "    largest_100_indexs = word_vec.argsort()[::-1][:100]\n",
    "    for rank, index in enumerate(largest_100_indexs):\n",
    "        topic_with_proba = word2topic.topics.get_or_create(rank)\n",
    "        topic_with_proba.proba = word_vec[index]\n",
    "        topic_with_proba.topic_code = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open ('dumps/full_dict.dump', 'wb') as f:\n",
    "    f.write(parse_dict.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary = Dictionary()\n",
    "with open ('dumps/dict.dump', 'rb') as f:\n",
    "    dictionary.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
