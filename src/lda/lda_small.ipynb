{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import time\n",
    "from functools import reduce\n",
    "from sklearn.decomposition  import LatentDirichletAllocation \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlargest(array: np.ndarray, n: int) -> list:\n",
    "    \"\"\"\n",
    "    找一个数组中前K大的数，当然是堆排序啦\n",
    "    \"\"\"\n",
    "    return array.argsort()[::-1][:n]\n",
    "\n",
    "def find_doc_from_verb(feature_names, doc2verb_matrix, titles, i):\n",
    "    word = feature_names[i]\n",
    "    print('word: {0} \\n'.format(word))\n",
    "    word_vec = doc2verb_matrix[:, i]\n",
    "    largest = nlargest(word_vec, 20)\n",
    "    s = set()\n",
    "    ten_largest = []\n",
    "    for n in largest:\n",
    "        if titles[n] not in s:\n",
    "            s.add(titles[n])\n",
    "            ten_largest.append(titles[n])\n",
    "        if len(ten_largest) == 11:\n",
    "            break\n",
    "    for j in ten_largest:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "excel = pd.ExcelFile(r'.\\..\\..\\data\\爬虫数据.xlsx')\n",
    "sheets = excel.sheet_names\n",
    "docs = []\n",
    "for name in sheets:\n",
    "    doc = {'name': name, 'title': [], 'texts': []}\n",
    "    df = excel.parse(name)\n",
    "    for _, row in df.iterrows():\n",
    "        if type(row.desc) is str:\n",
    "            doc['title'].append(row.title)\n",
    "            doc['texts'].append(row.desc)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load stopwords\n",
    "stopwords_dir_path = r'C:\\Users\\zjxua\\GitHub\\CAS-NLP\\data\\stopwords'\n",
    "stopwords_filelist = [os.path.join(stopwords_dir_path, p) for p in os.listdir(stopwords_dir_path)]\n",
    "stopwords = list(reduce(lambda x, y: x + y, [open(p).read().split('\\n') for p in stopwords_filelist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zjxua\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.766 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 10688)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(tokenizer=lambda text: jieba.lcut(text), max_df=0.9, min_df=20, stop_words=stopwords)\n",
    "\n",
    "all_texts  = list(reduce(lambda x, y: x + y, [doc['texts'] for doc in docs]))\n",
    "all_titles = list(reduce(lambda x, y: x + y, [doc['title'] for doc in docs]))\n",
    "\n",
    "tf_matrix = tf_vectorizer.fit_transform(all_texts)\n",
    "print(tf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "docres = lda.fit_transform(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部标题:\t 7352 \n",
      "话题 * 词:\t (10, 10688) \n",
      "文档 * 话题:\t (7352, 10) \n",
      "文档 * 词:\t (7352, 10688) \n"
     ]
    }
   ],
   "source": [
    "doc2verb = docres.dot(lda.components_)\n",
    "print('全部标题:\\t {0} '.format(len(all_titles)))\n",
    "print('话题 * 词:\\t {0} '.format(lda.components_.shape))\n",
    "print('文档 * 话题:\\t {0} '.format(docres.shape))\n",
    "print('文档 * 词:\\t {0} '.format(doc2verb.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 中西部 \n",
      "\n",
      "中央城市工作会议：2020年完成棚户区改造\n",
      "３年内农业信贷担保体系框架将覆盖全国\n",
      "京津冀交通规划近期印发深入推进区域运输一体化\n",
      "助力区域经济国家级经开区谋转型升级\n",
      "城市发展着力提高持续性宜居性\n",
      "北京林业大学与鲁能集团展开全方位深度合作\n",
      "鲁能公布新版发展战略聘贝克汉姆担任形象大使\n",
      "依托园区转化创新成果涞水产业新城对接京津\n",
      "科技部将再建山东半岛等一批国家自创区\n",
      "非首都功能批发市场疏解升级见成效\n",
      "机器人产业五年路线图出炉 支持企业直接融资并购\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "find_doc_from_verb(tf_vectorizer.get_feature_names(), doc2verb, all_titles, 1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    now = time.time()\n",
    "    i = int(input('输入一个数字'))\n",
    "    find_doc_from_verb(tf_vectorizer.get_feature_names(), doc2verb, all_titles, i)\n",
    "    print()\n",
    "    print('耗时 ', time.time() - now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "104px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
